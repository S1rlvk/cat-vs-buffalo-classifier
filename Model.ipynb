{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UC75ZBU69nz",
        "outputId": "04dbda61-c10f-424f-fdab-a5c236a41545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import dataloader\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "ZvGr25Oi8eBQ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "OfcTVzCd9ReX"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "pBrdrli4-mTb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Train/cat:\", os.listdir('/content/animals_classifier/train/cat'))\n",
        "print(\"Train/buffalo:\", os.listdir('/content/animals_classifier/train/buffalo'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoZBlabtJ1kb",
        "outputId": "0040243d-94db-413a-f12e-e8eebb9c3812"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/cat: ['istockphoto-1325997570-612x612.jpg', 'istockphoto-134398339-612x612.jpg', 'istockphoto-104355461-612x612.jpg', 'istockphoto-1361394182-612x612.jpg']\n",
            "Train/buffalo: ['istockphoto-842356126-612x612.jpg', 'istockphoto-1445453358-612x612.jpg', 'istockphoto-490431110-612x612.jpg', 'istockphoto-842228162-640x640.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets"
      ],
      "metadata": {
        "id": "NPe-4qq7Kp5a"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def is_valid_image(path):\n",
        "    return '.ipynb_checkpoints' not in path and path.lower().endswith ((\".png\", \".jpg\", \".jpeg\"))"
      ],
      "metadata": {
        "id": "lnZD7SEsLEzx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Remove the unwanted hidden folder if it exists\n",
        "shutil.rmtree('/content/animals_classifier/train/.ipynb_checkpoints', ignore_errors=True)\n",
        "shutil.rmtree('/content/animals_classifier/val/.ipynb_checkpoints', ignore_errors=True)\n"
      ],
      "metadata": {
        "id": "7SYVZKk_Oiu9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import dataloader"
      ],
      "metadata": {
        "id": "jCu0LXXbM3xj"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ImageFolder(\n",
        "    root='/content/animals_classifier/train',\n",
        "    transform=transform,\n",
        "    is_valid_file=is_valid_image\n",
        ")\n"
      ],
      "metadata": {
        "id": "k6GrBfhRMX7D"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = ImageFolder(\n",
        "    root='/content/animals_classifier/val',\n",
        "    transform=transform,\n",
        "    is_valid_file=is_valid_image\n",
        ")\n"
      ],
      "metadata": {
        "id": "6ypMYO-tMyMM"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
        "val_loader   = DataLoader(val_data, batch_size=2, shuffle=False)\n",
        "\n",
        "print(\"Classes:\", train_data.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdjvgc-POos9",
        "outputId": "3fb9d033-1821-4118-a741-be906b6bf70e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['buffalo', 'cat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class animals_classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))   # 128 → 64\n",
        "        x = self.pool(F.relu(self.conv2(x)))   # 64 → 32\n",
        "        x = self.pool(F.relu(self.conv3(x)))   # 32 → 16\n",
        "        x = x.view(-1, 64 * 16 * 16)           # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "QqKQTWGKQYbn"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = animals_classifier()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZvErwDkOsHy",
        "outputId": "0a769d09-cd21-49e1-8977-f2eb295a45c0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "animals_classifier(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=16384, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch"
      ],
      "metadata": {
        "id": "cnjGoXvHPfqo"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def predict_image(image_path, model, class_names):\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Transform to match training\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = image.to(next(model.parameters()).device)\n",
        "        output = model(image)\n",
        "        probs = torch.softmax(output, dim=1)[0]  # corrected this line\n",
        "\n",
        "    # Show class probabilities\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name}: {probs[i].item():.4f}\")\n",
        "\n",
        "    predicted_index = torch.argmax(probs).item()\n",
        "    return class_names[predicted_index]\n"
      ],
      "metadata": {
        "id": "CeQ9IhexR0Bx"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion  = nn.CrossEntropyLoss()\n",
        "optimizer  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 20                   # tiny dataset → many epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        # forward + loss\n",
        "        logits = model(imgs)\n",
        "        loss   = criterion(logits, labels)\n",
        "\n",
        "        # backward + update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}  loss {running_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-g_tzGmWmIx",
        "outputId": "431f8fc7-e848-4106-e7a0-b90c999a7738"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20  loss 4.0682\n",
            "Epoch 2/20  loss 3.0653\n",
            "Epoch 3/20  loss 2.5645\n",
            "Epoch 4/20  loss 2.3917\n",
            "Epoch 5/20  loss 2.0215\n",
            "Epoch 6/20  loss 1.3226\n",
            "Epoch 7/20  loss 0.5594\n",
            "Epoch 8/20  loss 0.2771\n",
            "Epoch 9/20  loss 0.0621\n",
            "Epoch 10/20  loss 0.0046\n",
            "Epoch 11/20  loss 0.0015\n",
            "Epoch 12/20  loss 0.0003\n",
            "Epoch 13/20  loss 0.0001\n",
            "Epoch 14/20  loss 0.0000\n",
            "Epoch 15/20  loss 0.0000\n",
            "Epoch 16/20  loss 0.0000\n",
            "Epoch 17/20  loss 0.0000\n",
            "Epoch 18/20  loss 0.0000\n",
            "Epoch 19/20  loss 0.0000\n",
            "Epoch 20/20  loss 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Path to an image you want to classify\n",
        "image_path = '/content/animals_classifier/val/cat/istockphoto-1443562748-612x612.jpg'  # change to buffalo image if needed\n",
        "\n",
        "# 2. Get the class names from training data\n",
        "class_names = train_data.classes  # e.g., ['buffalo', 'cat']\n",
        "\n",
        "# 3. Run the prediction\n",
        "predicted_class = predict_image(image_path, model, class_names)\n",
        "\n",
        "# 4. Show the result\n",
        "print(f\"\\nPredicted: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw9KNHVOSWhV",
        "outputId": "2e1c5303-78be-4d79-862a-b6c663089477"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buffalo: 0.0000\n",
            "cat: 1.0000\n",
            "\n",
            "Predicted: cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLn_VJJKVyWu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}